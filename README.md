

---

#  **Proyecto de Multicore ‚Äì Web Scraping**

### *Cat√°logo Din√°mico de Videojuegos con Paralelismo y GitHub Pages*

![alt text](image-2.png)

---

## üì∏ Vista previa del proyecto

### üîπ **P√°gina principal**

![alt text](image.png)

### üîπ **Vista de detalle de juego**

![alt text](image-1.png)



---

#  **Descripci√≥n General**

Este proyecto combina **Web Scraping**, **Paralelismo**, **Procesamiento autom√°tico de datos** y **GitHub Pages** para generar un **cat√°logo din√°mico de videojuegos**, completamente actualizado cada 3 minutos.

Toda la informaci√≥n se almacena en JSON y luego se muestra en una p√°gina web moderna, elegante e interactiva.

---

#  **Problema Presentado**

En la industria del software, es com√∫n que existan plataformas que ofrecen productos (como videojuegos), pero:

###  Problemas identificados:

* La informaci√≥n de precios cambia constantemente.
* Los juegos aparecen en varias tiendas con descuentos diferentes.
* No existe un cat√°logo centralizado que recopile datos reales desde m√∫ltiples plataformas.
* Se debe procesar gran cantidad de URLs ‚Üí **costoso sin paralelismo**.


---

#  **Soluci√≥n Implementada**

Se construy√≥ un sistema completamente automatizado que:

### üîπ **1. Realiza Web Scraping en Paralelo**

Usa **tres niveles de paralelismo**:

1. **Primer nivel:** extracci√≥n de nombres
2. **Segundo nivel:** extracci√≥n de precios, descuentos e im√°genes
3. **Tercer nivel:** duraci√≥n aproximada (HowLongToBeat), calificaciones, formato, etc.

Esto permite procesar cientos de juegos en minutos.

---

### üîπ **2. Recopila datos de 3 plataformas reales**

* **Steam**
* **GOG**
* **GMG (Green Man Gaming)**



---

### üîπ **3. Actualiza GitHub autom√°ticamente**

Cada vez que el scraper finaliza:

```
git add results.json results.csv
git commit -m "Actualizar datos ..."
git push
```

Esto significa que **la p√°gina web SIEMPRE muestra datos recientes**.

---

### üîπ **4. Publicaci√≥n Autom√°tica con GitHub Pages**

GitHub Pages lee siempre el archivo:

```
index.html
```

Y la web se actualiza autom√°ticamente cuando `results.json` cambia.

---

#  **Procesamiento del Proyecto**

## üî∏ Flujo de Ejecuci√≥n

1. Se recolectan cientos de URLs por tienda (seeders)
2. Se procesan con **paralelismo controlado**
3. Se extraen datos reales de cada p√°gina
4. Utilizar los mejores precios recopilados
5. Se genera:

   * `results.json`
   * `results.csv`
6. Se mandan los datos autom√°ticamente a GitHub
7. La web se actualiza con la nueva informaci√≥n

---

#  **Tecnolog√≠as Utilizadas**

### **Backend / Scraper**

* Python 3
* `aiohttp` ‚Üí descargas en paralelo
* `aiofiles` ‚Üí guardado asincr√≥nico
* `BeautifulSoup4` + `lxml` ‚Üí parsing HTML
* `subprocess` ‚Üí push autom√°tico a GitHub
* Manejo de errores avanzado
* Expresiones regulares

### **Frontend**

* HTML5 + CSS3
* JavaScript Vanilla
* GitHub Pages (Deploy autom√°tico)

---

#  **P√°gina Web (Cat√°logo de Juegos)**

La web:

‚úî Filtra por tienda
‚úî Filtra por formato (Digital / F√≠sico)
‚úî Filtra por plataforma (PC, PS4, PS5, Xbox, etc.)
‚úî Ordena por precio, nombre, descuento, rating
‚úî Busca por nombre
‚úî Muestra detalles ampliados
‚úî Linkea a la tienda original

---

#  **C√≥mo Ejecutar el Scraper**

### 1Ô∏è‚É£ Instalar dependencias

```bash
pip install aiohttp aiofiles beautifulsoup4 lxml
```

### 2Ô∏è‚É£ Ejecutar

```bash
python datos.py
```

El programa:

* Corre indefinidamente
* Recolecta datos cada 3 minutos
* Hace `git push` autom√°ticamente

---

#  **C√≥mo Publicar la Web en GitHub Pages**

1. Subir el archivo **index.html**, `results.json` y el scraper a la ra√≠z del repositorio
2. Entrar a **Settings ‚Üí Pages**
3. Configurar

```
Source: main
Folder: /(root)
```

4. Guardar los cambios

La pagina web aparecere en:

```
https://danny7w7-coder.github.io/proyecto-multicore-web/
```

---

# üîß **Uso de GitHub paso a paso (para principiantes)**

### ‚úî Clonar repositorio

```bash
git clone https://github.com/usuario/repositorio.git
```

### ‚úî Agregar cambios

```bash
git add .
```

### ‚úî Guardar cambios

```bash
git commit -m "Mi actualizaci√≥n"
```

### ‚úî Subir a GitHub

```bash
git push
```

### ‚úî Descargar cambios remotos

```bash
git pull
```

---

# üë§ **Autores del Proyecto**

### **Valeria Rojas Barrantes**

Estudiante de Ingenier√≠a en Computacion ‚Äî TEC

### **Dylan M√©ndez Zamora**

Estudiante de Ingenier√≠a en Computacion ‚Äî TEC

### **Danny Gonz√°lez Molina**

Estudiante de Ingenier√≠a en Computacion ‚Äî TEC

---

#  **Conclusi√≥n**

Este proyecto demuestra c√≥mo combinar:

* **Paralelismo**
* **Web Scraping**
* **Automatizaci√≥n**
* **Publicaci√≥n continua**
* **Frontend din√°mico**
